{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torch.cuda\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand written digit recognition with Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create Resnet with PyTorch use it on MNIST dataset. MNIST is collection of grayscale images of written digits with 10 classes (digits 0-9). The images have all the same resolution 28x28 pixels. We won't do any Exploratory Data Analysis, because the dataset shouldn't contain anything tricky (different sizes, channels,...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: https://pjreddie.com/projects/mnist-in-csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data - PyTorch Dataset, Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch Dataset and DataLoader classes for storing and iterating over data. The catch here is we won't directly store the training values in dataset. Instead, the Dataset object stores IDs and labels as a reference to the actual data. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv(f'{PATH}mnist_train.csv', header=None)\n",
    "mnist_valid = pd.read_csv(f'{PATH}mnist_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = mnist_train.drop(columns=[0])\n",
    "valid_x = mnist_valid.drop(columns=[0])\n",
    "train_y = mnist_train.iloc[:,0]\n",
    "valid_y = mnist_valid.iloc[:,0]\n",
    "train_idx = mnist_train.index\n",
    "valid_idx = mnist_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_map = {idx:label for label, idx in zip(train_y, train_idx)}\n",
    "valid_id_map = {idx:label for label, idx in zip(valid_y, valid_idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, idxs, id_map, xs_df):\n",
    "        self.id_map = id_map\n",
    "        self.idxs = idxs\n",
    "        self.xs_df = xs_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.id_map)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_ID = self.idxs[idx]\n",
    "        x = self.xs_df.iloc[real_ID]\n",
    "        x = np.reshape(np.array(x),(28,28))\n",
    "        y = self.id_map[real_ID]\n",
    "        return x.astype(np.float32),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST_Dataset(train_idx,train_id_map,train_x)\n",
    "valid_dataset = MNIST_Dataset(valid_idx,valid_id_map,valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, bs, True)\n",
    "valid_dataloader = DataLoader(valid_dataset, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready both for training and evaluation, we can proceed to create the ResNet itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvWithBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,3,stride,padding=1)\n",
    "        self.batch = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.conv(inp)\n",
    "        return self.batch(F.relu(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(ConvWithBatchNorm):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, layers, classes, p=0.5):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=layers[0],kernel_size=5,padding=2)\n",
    "        self.bns = nn.ModuleList([ConvWithBatchNorm(layers[i], layers[i+1], stride=2) \n",
    "                                  for i in range(len(layers)-1)])\n",
    "        self.res1s = nn.ModuleList([ResNetLayer(layers[i+1], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.res2s = nn.ModuleList([ResNetLayer(layers[i+1], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.out = nn.Linear(layers[-1], classes)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        for b,r1,r2 in zip(self.bns, self.res1s, self.res2s):\n",
    "            out = r2(r1(b(out)))\n",
    "        out = F.adaptive_max_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop(out)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement our own train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(m, train_dl, valid_dl, opt, loss_fn, epochs):\n",
    "    def getLoss(valid=False, loss_fn=loss_fn):\n",
    "        data = train_dl if not valid else valid_dl\n",
    "        num_batch, loss_avg = 0,0\n",
    "        m.eval()\n",
    "        for x,y in data:\n",
    "            x = V(x[:,None,:,:]).cuda()\n",
    "            y = V(y).cuda()\n",
    "            out = m(x)\n",
    "            loss_avg += float(loss_fn(out,y))\n",
    "            num_batch += 1\n",
    "        m.train()\n",
    "        return loss_avg/num_batch\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for x,y in train_dl:\n",
    "            # Add singleton dimension (channel)\n",
    "            x = V(x[:,None,:,:]).cuda()\n",
    "            y = V(y).cuda()\n",
    "            out = m(x)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f'Epoch {e+1} \\n \\\n",
    "                Train loss: {getLoss()} \\n \\\n",
    "                Validation loss: {getLoss(True)} \\n \\\n",
    "                Validation accuracy: {getLoss(True,accuracy)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-5\n",
    "m = ResNet([10,20,40,80,160],10).cuda()\n",
    "opt = optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,y):\n",
    "    pred = pred.max(1)[1]\n",
    "    acc = 0\n",
    "    for i,p in enumerate(pred):\n",
    "        acc += float((p == y[i]))\n",
    "    return acc/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "                 Train loss: 0.06263531989523216 \n",
      "                 Validation loss: 0.06613515988941406 \n",
      "                 Validation accuracy: 97.80055732484077%\n",
      "Epoch 2 \n",
      "                 Train loss: 0.0282764425521085 \n",
      "                 Validation loss: 0.0344614742477988 \n",
      "                 Validation accuracy: 98.86544585987261%\n",
      "Epoch 3 \n",
      "                 Train loss: 0.02393992072038813 \n",
      "                 Validation loss: 0.03023368407301842 \n",
      "                 Validation accuracy: 99.0047770700637%\n",
      "Epoch 4 \n",
      "                 Train loss: 0.02597112249114366 \n",
      "                 Validation loss: 0.03985336769348497 \n",
      "                 Validation accuracy: 98.82563694267516%\n",
      "Epoch 5 \n",
      "                 Train loss: 0.01897445623911838 \n",
      "                 Validation loss: 0.033840199469760725 \n",
      "                 Validation accuracy: 98.96496815286623%\n",
      "Epoch 6 \n",
      "                 Train loss: 0.021983704160350854 \n",
      "                 Validation loss: 0.037374936162855976 \n",
      "                 Validation accuracy: 98.84554140127389%\n",
      "Epoch 7 \n",
      "                 Train loss: 0.011018465425985963 \n",
      "                 Validation loss: 0.025685449552004504 \n",
      "                 Validation accuracy: 99.17396496815286%\n",
      "Epoch 8 \n",
      "                 Train loss: 0.01635826736100828 \n",
      "                 Validation loss: 0.03333778086171788 \n",
      "                 Validation accuracy: 99.03463375796179%\n",
      "Epoch 9 \n",
      "                 Train loss: 0.011639401272955988 \n",
      "                 Validation loss: 0.025664823177229068 \n",
      "                 Validation accuracy: 99.18391719745223%\n",
      "Epoch 10 \n",
      "                 Train loss: 0.01109881283266585 \n",
      "                 Validation loss: 0.02753410728019514 \n",
      "                 Validation accuracy: 99.21377388535032%\n",
      "Epoch 11 \n",
      "                 Train loss: 0.013673387861042134 \n",
      "                 Validation loss: 0.04307258046072 \n",
      "                 Validation accuracy: 98.84554140127389%\n",
      "Epoch 12 \n",
      "                 Train loss: 0.01291625611364905 \n",
      "                 Validation loss: 0.029728032648563385 \n",
      "                 Validation accuracy: 99.12420382165605%\n",
      "Epoch 13 \n",
      "                 Train loss: 0.0052678105332005 \n",
      "                 Validation loss: 0.028850860537806895 \n",
      "                 Validation accuracy: 99.30334394904459%\n",
      "Epoch 14 \n",
      "                 Train loss: 0.007030955291410753 \n",
      "                 Validation loss: 0.030877064462679966 \n",
      "                 Validation accuracy: 99.10429936305732%\n",
      "Epoch 15 \n",
      "                 Train loss: 0.006211870848370005 \n",
      "                 Validation loss: 0.03086613531515097 \n",
      "                 Validation accuracy: 99.20382165605095%\n",
      "Epoch 16 \n",
      "                 Train loss: 0.0032867020937298406 \n",
      "                 Validation loss: 0.021473856298786818 \n",
      "                 Validation accuracy: 99.35310509554141%\n",
      "Epoch 17 \n",
      "                 Train loss: 0.006530025167696511 \n",
      "                 Validation loss: 0.027179312554134683 \n",
      "                 Validation accuracy: 99.30334394904459%\n",
      "Epoch 18 \n",
      "                 Train loss: 0.007112558879482467 \n",
      "                 Validation loss: 0.034936525141167796 \n",
      "                 Validation accuracy: 99.12420382165605%\n",
      "Epoch 19 \n",
      "                 Train loss: 0.0030236982944995354 \n",
      "                 Validation loss: 0.02598836866154033 \n",
      "                 Validation accuracy: 99.23367834394905%\n",
      "Epoch 20 \n",
      "                 Train loss: 0.003396553056898402 \n",
      "                 Validation loss: 0.030026493819466062 \n",
      "                 Validation accuracy: 99.26353503184714%\n"
     ]
    }
   ],
   "source": [
    "train(m,train_dataloader,valid_dataloader,opt,loss_fn,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
